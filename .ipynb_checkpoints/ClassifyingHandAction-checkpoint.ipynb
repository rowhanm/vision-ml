{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI-GA 3033-012 Vision meets Machine Learning Assignment 3\n",
    "\n",
    "Rohan Mahadev - rm5310\n",
    "\n",
    "Part 2 - Classifying video into one of 4 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first observations while trying to solve this problem is:\n",
    "1. If we're trying to classify the whole video into one category, we need to figure out what makes each category distinct from each other.\n",
    "2. Looking at the videos, the main distinction is that the objects in different categories are different. For e.g., there is a chess board and chess pieces in the \"chess\" videos, playing cards in the \"cards\" category etc.\n",
    "3. So what I think we need is an object classifier for the frames, and the temporal aspect of the videos don't play a big role.\n",
    "4. Also, the hand movements in the 4 categories are pretty much the same, so I don't think visually, using the hand masks gives us any useful information about the category of video, so I will stick with the RGB frames.\n",
    "5. This is just my initial observation and based on results for this we can decide on further approaches.\n",
    "6. So I'll use the backbone of the MaskRCNN model, and finetune it to classify every RGB frame into the 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from torchvision import transforms\n",
    "# transform = transforms.Compose([transforms.ToTensor()])  # Convert image to PyTorch Tensor\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datapoints from the subcategories into 4 classes\n",
    "#### Done using bash commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mv egohands/CLASSIFICATION_DATA/CARDS_*/* egohands/CLASSIFICATION_DATA/cards/\n",
    "# !mv egohands/CLASSIFICATION_DATA/CHESS_*/* egohands/CLASSIFICATION_DATA/chess/\n",
    "# !mv egohands/CLASSIFICATION_DATA/PUZZLE_*/* egohands/CLASSIFICATION_DATA/puzzle/\n",
    "# !mv egohands/CLASSIFICATION_DATA/JENGA_*/* egohands/CLASSIFICATION_DATA/jenga/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=torchvision.datasets.ImageFolder(root=\"./egohands/CLASSIFICATION_DATA/\",transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cards', 'chess', 'jenga', 'puzzle']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using same model, but only using output of backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(num_classes=2)\n",
    "state_dict = torch.load('finetuned_better_2.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that the backbone is a Resnet50 and since the backbone wasn't trained during finetuning, using the resnet50 from torchvision would be equivalent to using the intermediate activations from this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So I'll be using the torchvision model for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = model.backbone.body.forward(dataset[0][0].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because doing the above is not too code-friendly :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above is equivalent to using the backbone of the Mask-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fc = torch.nn.Linear(in_features=2048, out_features=4, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My approach for this task initially is to run every RGB frame from the videos through the backbone and classify the frame into one of the 4 classes directly. This will act as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this we will finetune only the fully connected layer of the ResNet to classify the frame into one of the 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in classifier.fc.parameters():\n",
    "    params.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = correct.float()/preds.shape[0]\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            fx = model(x)\n",
    "\n",
    "            loss = criterion(fx, y)\n",
    "            \n",
    "            acc = calculate_accuracy(fx, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, sched=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch_idx,(x, y) in enumerate(iterator):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        fx = model(x)\n",
    "        \n",
    "        loss = criterion(fx, y)\n",
    "        \n",
    "        acc = calculate_accuracy(fx, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        lr_sched_test = scheduler.get_lr()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(classifier.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_loader), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 0]\n",
      "Epoch: 01 | Epoch Time: 2m 46s\n",
      "\tTrain Loss: 0.184 | Train Acc: 96.53%\n",
      "\t Val. Loss: 0.009 |  Val. Acc: 99.88%\n",
      "[EPOCH 1]\n",
      "Epoch: 02 | Epoch Time: 2m 27s\n",
      "\tTrain Loss: 0.004 | Train Acc: 99.97%\n",
      "\t Val. Loss: 0.006 |  Val. Acc: 99.88%\n",
      "[EPOCH 2]\n",
      "Epoch: 03 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.003 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.003 |  Val. Acc: 99.88%\n",
      "[EPOCH 3]\n",
      "Epoch: 04 | Epoch Time: 2m 1s\n",
      "\tTrain Loss: 0.002 | Train Acc: 99.97%\n",
      "\t Val. Loss: 0.005 |  Val. Acc: 99.88%\n",
      "[EPOCH 4]\n",
      "Epoch: 05 | Epoch Time: 1m 43s\n",
      "\tTrain Loss: 0.003 | Train Acc: 99.94%\n",
      "\t Val. Loss: 0.007 |  Val. Acc: 99.88%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(classifier, train_loader, optimizer, criterion, scheduler)\n",
    "    valid_loss, valid_acc = evaluate(classifier, validation_loader, criterion)\n",
    "    print(\"[EPOCH \" + str(epoch)+\"]\")\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(classifier.state_dict(), 'hands-classifier-2.pth')\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The backbone is now finetuned, so now we evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cards', 'chess', 'jenga', 'puzzle']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation:\n",
    "\n",
    "## The procedure is that for every frame we calculate the class probabilities and keep a running average of all the class probabilities for every frame\n",
    "\n",
    "## At the end of running the model through every frame, the max avg. probability will be our final prediction for the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = os.listdir(\"./egohands/all_vids/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_d = torch.load(\"hands-classifier-2.pth\")\n",
    "classifier.load_state_dict(state_d)\n",
    "classifier.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...evaluating chess_courtyard_B_T.mp4 score-0/0\n",
      "...evaluating puzzle_livingroom_H_S.mp4 score-1/1\n",
      "...evaluating puzzle_courtyard_T_H.mp4 score-2/2\n",
      "...evaluating cards_courtyard_S_H.mp4 score-3/3\n",
      "...evaluating jenga_courtyard_B_H.mp4 score-4/4\n",
      "...evaluating cards_livingroom_T_B.mp4 score-4/5\n",
      "...evaluating jenga_office_S_B.mp4 score-5/6\n",
      "...evaluating cards_livingroom_H_S.mp4 score-5/7\n",
      "...evaluating puzzle_courtyard_B_S.mp4 score-6/8\n",
      "...evaluating cards_livingroom_S_H.mp4 score-7/9\n",
      "...evaluating cards_office_T_H.mp4 score-8/10\n",
      "...evaluating chess_courtyard_T_B.mp4 score-9/11\n",
      "...evaluating jenga_livingroom_S_T.mp4 score-10/12\n",
      "...evaluating jenga_livingroom_T_S.mp4 score-10/13\n",
      "...evaluating jenga_courtyard_T_S.mp4 score-10/14\n",
      "...evaluating chess_livingroom_H_T.mp4 score-10/15\n",
      "...evaluating cards_courtyard_H_S.mp4 score-11/16\n",
      "...evaluating jenga_courtyard_H_B.mp4 score-12/17\n",
      "...evaluating chess_office_B_S.mp4 score-12/18\n",
      "...evaluating jenga_livingroom_H_B.mp4 score-13/19\n",
      "...evaluating jenga_livingroom_B_H.mp4 score-13/20\n",
      "...evaluating puzzle_office_H_B.mp4 score-13/21\n",
      "...evaluating cards_courtyard_T_B.mp4 score-14/22\n",
      "...evaluating jenga_office_B_S.mp4 score-15/23\n",
      "...evaluating chess_courtyard_H_S.mp4 score-15/24\n",
      "...evaluating chess_office_H_T.mp4 score-16/25\n",
      "...evaluating puzzle_livingroom_B_T.mp4 score-17/26\n",
      "...evaluating chess_livingroom_B_S.mp4 score-18/27\n",
      "...evaluating puzzle_office_B_H.mp4 score-19/28\n",
      "...evaluating cards_office_H_T.mp4 score-20/29\n",
      "...evaluating cards_office_S_B.mp4 score-21/30\n",
      "...evaluating puzzle_office_T_S.mp4 score-22/31\n",
      "...evaluating chess_courtyard_S_H.mp4 score-23/32\n",
      "...evaluating cards_office_B_S.mp4 score-24/33\n",
      "...evaluating puzzle_courtyard_H_T.mp4 score-25/34\n",
      "...evaluating chess_office_T_H.mp4 score-26/35\n",
      "...evaluating jenga_office_T_H.mp4 score-27/36\n",
      "...evaluating chess_livingroom_S_B.mp4 score-27/37\n",
      "...evaluating chess_livingroom_T_H.mp4 score-28/38\n",
      "...evaluating puzzle_office_S_T.mp4 score-29/39\n",
      "...evaluating puzzle_courtyard_S_B.mp4 score-30/40\n",
      "...evaluating chess_office_S_B.mp4 score-31/41\n",
      "...evaluating jenga_courtyard_S_T.mp4 score-32/42\n",
      "...evaluating jenga_office_H_T.mp4 score-32/43\n",
      "...evaluating puzzle_livingroom_S_H.mp4 score-32/44\n",
      "...evaluating cards_livingroom_B_T.mp4 score-33/45\n",
      "...evaluating cards_courtyard_B_T.mp4 score-34/46\n",
      "...evaluating puzzle_livingroom_T_B.mp4 score-35/47\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "vidcount = 0\n",
    "for vid in vids:\n",
    "    print(\"...evaluating\",vid+\" score-\"+str(correct)+\"/\"+str(vidcount))\n",
    "    capture = cv2.VideoCapture(os.path.join(\"./egohands/all_vids\", vid))\n",
    "    frame_count = 0\n",
    "    result_probs = [0]*4\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        # Bail out when the video file ends\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2_im = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        pil_im = Image.fromarray(cv2_im)\n",
    "        img = transforms(pil_im)\n",
    "        probs = classifier(img.to(device).unsqueeze(0))\n",
    "        pred_idx = probs.argmax()\n",
    "        result_probs[pred_idx] += 1\n",
    "        frame_count += 1\n",
    "    pred = result_probs.index(max(result_probs))\n",
    "    classname = dataset.classes[pred]\n",
    "    true_label = vid[0:vid.find(\"_\")]\n",
    "    if classname == true_label:\n",
    "        correct += 1\n",
    "    vidcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(correct/vidcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This approach gives us an accuracy of 75% \n",
    "## but takes a lot of time to process, since we run our feature extractor model on every frame\n",
    "## I think the score is low because there are frames in the video which act as outliers which drastically change the average prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the next attempt will be to process only 1 in 100 frames (which potentially would smoothen the predictions and hopefully get rid of the outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_d = torch.load(\"hands-classifier-1.pth\")\n",
    "classifier.load_state_dict(state_d)\n",
    "classifier.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = os.listdir(\"./egohands/all_vids/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...evaluating chess_courtyard_B_T.mp4 score-0/0\n",
      "...evaluating puzzle_livingroom_H_S.mp4 score-1/1\n",
      "...evaluating puzzle_courtyard_T_H.mp4 score-2/2\n",
      "...evaluating cards_courtyard_S_H.mp4 score-3/3\n",
      "...evaluating jenga_courtyard_B_H.mp4 score-4/4\n",
      "...evaluating cards_livingroom_T_B.mp4 score-4/5\n",
      "...evaluating jenga_office_S_B.mp4 score-5/6\n",
      "...evaluating cards_livingroom_H_S.mp4 score-5/7\n",
      "...evaluating puzzle_courtyard_B_S.mp4 score-6/8\n",
      "...evaluating cards_livingroom_S_H.mp4 score-7/9\n",
      "...evaluating cards_office_T_H.mp4 score-8/10\n",
      "...evaluating chess_courtyard_T_B.mp4 score-9/11\n",
      "...evaluating jenga_livingroom_S_T.mp4 score-10/12\n",
      "...evaluating jenga_livingroom_T_S.mp4 score-10/13\n",
      "...evaluating jenga_courtyard_T_S.mp4 score-10/14\n",
      "...evaluating chess_livingroom_H_T.mp4 score-10/15\n",
      "...evaluating cards_courtyard_H_S.mp4 score-10/16\n",
      "...evaluating jenga_courtyard_H_B.mp4 score-11/17\n",
      "...evaluating chess_office_B_S.mp4 score-11/18\n",
      "...evaluating jenga_livingroom_H_B.mp4 score-12/19\n",
      "...evaluating jenga_livingroom_B_H.mp4 score-12/20\n",
      "...evaluating puzzle_office_H_B.mp4 score-12/21\n",
      "...evaluating cards_courtyard_T_B.mp4 score-13/22\n",
      "...evaluating jenga_office_B_S.mp4 score-14/23\n",
      "...evaluating chess_courtyard_H_S.mp4 score-14/24\n",
      "...evaluating chess_office_H_T.mp4 score-15/25\n",
      "...evaluating puzzle_livingroom_B_T.mp4 score-16/26\n",
      "...evaluating chess_livingroom_B_S.mp4 score-17/27\n",
      "...evaluating puzzle_office_B_H.mp4 score-18/28\n",
      "...evaluating cards_office_H_T.mp4 score-19/29\n",
      "...evaluating cards_office_S_B.mp4 score-20/30\n",
      "...evaluating puzzle_office_T_S.mp4 score-21/31\n",
      "...evaluating chess_courtyard_S_H.mp4 score-22/32\n",
      "...evaluating cards_office_B_S.mp4 score-23/33\n",
      "...evaluating puzzle_courtyard_H_T.mp4 score-24/34\n",
      "...evaluating chess_office_T_H.mp4 score-25/35\n",
      "...evaluating jenga_office_T_H.mp4 score-26/36\n",
      "...evaluating chess_livingroom_S_B.mp4 score-26/37\n",
      "...evaluating chess_livingroom_T_H.mp4 score-27/38\n",
      "...evaluating puzzle_office_S_T.mp4 score-28/39\n",
      "...evaluating puzzle_courtyard_S_B.mp4 score-29/40\n",
      "...evaluating chess_office_S_B.mp4 score-30/41\n",
      "...evaluating jenga_courtyard_S_T.mp4 score-31/42\n",
      "...evaluating jenga_office_H_T.mp4 score-31/43\n",
      "...evaluating puzzle_livingroom_S_H.mp4 score-31/44\n",
      "...evaluating cards_livingroom_B_T.mp4 score-32/45\n",
      "...evaluating cards_courtyard_B_T.mp4 score-33/46\n",
      "...evaluating puzzle_livingroom_T_B.mp4 score-34/47\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "vidcount = 0\n",
    "frame_rate = 100\n",
    "for vid in vids:\n",
    "    print(\"...evaluating\",vid+\" score-\"+str(correct)+\"/\"+str(vidcount))\n",
    "    capture = cv2.VideoCapture(os.path.join(\"./egohands/all_vids\", vid))\n",
    "    frame_count = 0\n",
    "    result_probs = [0]*4\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        # Bail out when the video file ends\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_rate == 0:\n",
    "            cv2_im = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "            pil_im = Image.fromarray(cv2_im)\n",
    "            img = transforms(pil_im)\n",
    "            probs = classifier(img.to(device).unsqueeze(0))\n",
    "            pred_idx = probs.argmax()\n",
    "            result_probs[pred_idx] += 1\n",
    "        frame_count += 1\n",
    "    pred = result_probs.index(max(result_probs))\n",
    "    classname = dataset.classes[pred]\n",
    "    true_label = vid[0:vid.find(\"_\")]\n",
    "    if classname == true_label:\n",
    "        correct += 1\n",
    "    vidcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.91666666666666\n"
     ]
    }
   ],
   "source": [
    "print(35/48 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This was not helpful because we seemed to have gotten rid of the good frames and included more outliers. So we need a more structured approach to get rid of the outliers and only include the representative frames from the video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As discussed with Prof. Geiger, I am going to use Submodular optimization for data subset selection:\n",
    "### I wrote a few papers on this topic previously. - [[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8659119) [[2]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658965)\n",
    "\n",
    "### The basic idea is as follows: \n",
    "1. A video is a set of frames.\n",
    "2. Each frame has some amount of information(sometimes referred to as \"utility\" which can be calculated using certain set functions like Facility Location/Disparity Min etc.).\n",
    "3. But the total information in the video is not equal to the sum of information from every frame.\n",
    "4. This is because the information is distributed unevenly, with some frames having more information and some having very little.\n",
    "5. These kinds of distributions are called submodular functions.\n",
    "6. A good thing about submodular functions is that we can use mathematical optimization techniques to find a subset of the entire set which maximizes the utility function.\n",
    "7. A utility function which conveys the concept of \"representation\" is Facility Location.\n",
    "8. In this context, we find the subset of frames from our video which are the most \"representative\" using this Facility Location function\n",
    "9. This will potentially get rid of the outliers and we can run our classifier on only the most representative frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I, along with my collaborators at my previous lab, wrote an open source C++ library which does this submodular optimization on videos called \"vis-dss\" which gives us a subset of frames from the video which are the most representative. The code can be found here - https://github.com/rowhanm/vis-dss\n",
    "\n",
    "# Let me know if running the code is an issue, I've tried to make it as user friendly as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What this code does is essentially create a \"highlights\" video from our original video with only the representative frames which would hopefully remove all the outliers which were making our predictions bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create subsets (BASH script):\n",
    "# for file in /scratch/rm5310/myjupyter/Vision-ML/egohands/all_vids/*\n",
    "# do\n",
    "#   fname=$(echo $file |  sed -r \"s/.+\\/(.+)\\..+/\\1/\")\n",
    "#   echo $fname\n",
    "#   /scratch/rm5310/myjupyter/Vision-ML/vis-dss/build/SimpleVideoSummExample -videoFile $file -videoSaveFile /scratch/rm5310/myjupyter/Vision-ML/egohands/subset_vids/$fname.avi -summaryModel 0 -segmentType 0 -summaryAlgo 0 -budget 10\n",
    "# done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highlights video stored in a different folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = os.listdir(\"./egohands/subset_vids/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the same evaluation code on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...evaluating puzzle_livingroom_S_H.avi score-0/0\n",
      "...evaluating chess_courtyard_B_T.avi score-1/1\n",
      "...evaluating chess_livingroom_H_T.avi score-2/2\n",
      "...evaluating puzzle_office_H_B.avi score-3/3\n",
      "...evaluating chess_courtyard_S_H.avi score-4/4\n",
      "...evaluating chess_office_B_S.avi score-5/5\n",
      "...evaluating chess_courtyard_T_B.avi score-6/6\n",
      "...evaluating jenga_office_H_T.avi score-7/7\n",
      "...evaluating cards_livingroom_H_S.avi score-8/8\n",
      "...evaluating cards_office_T_H.avi score-9/9\n",
      "...evaluating puzzle_courtyard_T_H.avi score-10/10\n",
      "...evaluating jenga_courtyard_T_S.avi score-11/11\n",
      "...evaluating jenga_office_T_H.avi score-12/12\n",
      "...evaluating jenga_livingroom_T_S.avi score-13/13\n",
      "...evaluating cards_livingroom_T_B.avi score-14/14\n",
      "...evaluating chess_office_H_T.avi score-15/15\n",
      "...evaluating chess_office_T_H.avi score-16/16\n",
      "...evaluating cards_courtyard_H_S.avi score-17/17\n",
      "...evaluating puzzle_livingroom_T_B.avi score-18/18\n",
      "...evaluating puzzle_courtyard_B_S.avi score-19/19\n",
      "...evaluating chess_livingroom_B_S.avi score-20/20\n",
      "...evaluating jenga_livingroom_B_H.avi score-21/21\n",
      "...evaluating puzzle_livingroom_H_S.avi score-22/22\n",
      "...evaluating chess_livingroom_S_B.avi score-23/23\n",
      "...evaluating cards_office_H_T.avi score-24/24\n",
      "...evaluating chess_courtyard_H_S.avi score-25/25\n",
      "...evaluating jenga_courtyard_H_B.avi score-26/26\n",
      "...evaluating jenga_livingroom_H_B.avi score-27/27\n",
      "...evaluating cards_courtyard_T_B.avi score-28/28\n",
      "...evaluating chess_livingroom_T_H.avi score-29/29\n",
      "...evaluating jenga_office_B_S.avi score-30/30\n",
      "...evaluating cards_office_S_B.avi score-31/31\n",
      "...evaluating cards_livingroom_S_H.avi score-32/32\n",
      "...evaluating cards_livingroom_B_T.avi score-33/33\n",
      "...evaluating cards_office_B_S.avi score-34/34\n",
      "...evaluating cards_courtyard_S_H.avi score-35/35\n",
      "...evaluating puzzle_office_T_S.avi score-36/36\n",
      "...evaluating chess_office_S_B.avi score-37/37\n",
      "...evaluating jenga_courtyard_S_T.avi score-38/38\n",
      "...evaluating puzzle_office_B_H.avi score-39/39\n",
      "...evaluating puzzle_livingroom_B_T.avi score-40/40\n",
      "...evaluating jenga_courtyard_B_H.avi score-41/41\n",
      "...evaluating puzzle_office_S_T.avi score-42/42\n",
      "...evaluating puzzle_courtyard_H_T.avi score-43/43\n",
      "...evaluating jenga_livingroom_S_T.avi score-44/44\n",
      "...evaluating cards_courtyard_B_T.avi score-45/45\n",
      "...evaluating jenga_office_S_B.avi score-46/46\n",
      "...evaluating puzzle_courtyard_S_B.avi score-47/47\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "vidcount = 0\n",
    "for vid in vids:\n",
    "    print(\"...evaluating\",vid+\" score-\"+str(correct)+\"/\"+str(vidcount))\n",
    "    capture = cv2.VideoCapture(os.path.join(\"./egohands/subset_vids\", vid))\n",
    "    frame_count = 0\n",
    "    result_probs = [0]*4\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        # Bail out when the video file ends\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2_im = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        pil_im = Image.fromarray(cv2_im)\n",
    "        img = transforms(pil_im)\n",
    "        probs = classifier(img.to(device).unsqueeze(0))\n",
    "        pred_idx = probs.argmax()\n",
    "        result_probs[pred_idx] += 1\n",
    "        frame_count += 1\n",
    "    pred = result_probs.index(max(result_probs))\n",
    "    classname = dataset.classes[pred]\n",
    "    true_label = vid[0:vid.find(\"_\")]\n",
    "    if classname == true_label:\n",
    "        correct += 1\n",
    "    vidcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "print(correct/vidcount * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsurprisingly, this gives us 100% accuracy on the video classification as we have gotten rid of all outliers and our initial assumption was proven to be mostly correct.\n",
    "# According to me this approach is better than using a 1D conv or an LSTM because this is much more computationally cheap and the subset selection code runs entirely on the CPU with very little complex calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
